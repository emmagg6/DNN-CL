Batch Norm running_mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Batch Norm running_var: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
Batch Norm running_mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Batch Norm running_var: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
Batch Norm running_mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Batch Norm running_var: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
Batch Norm running_mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Batch Norm running_var: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
Batch Norm running_mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Batch Norm running_var: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
Epoch 0
Epoch 1
Epoch 2
Epoch 3
Epoch 4
Epoch 5
Epoch 6
Epoch 7
Epoch 8
Epoch 9
Epoch 10
Layer 0 - Weight Sample: tensor([[-0.0591, -0.0319, -0.0286,  ..., -0.0415,  0.0303, -0.0124],
        [ 0.0007,  0.0210,  0.0313,  ...,  0.0362,  0.0148,  0.0442],
        [-0.0754, -0.0268,  0.0041,  ...,  0.0477, -0.0146,  0.0052],
        [ 0.0125, -0.0234,  0.0011,  ..., -0.0001, -0.0081, -0.0029],
        [-0.0467, -0.0294, -0.0791,  ...,  0.0302, -0.0855,  0.0813]])
Layer 0 - Batch Norm Mean: tensor([  2.6621,  -7.6651,   1.3871,  -0.3651,   4.4279,   1.3576,   2.1115,
          0.8487,   0.5747,  -2.6372,  -1.9347,   2.2837,  -0.1814,  -2.8131,
         -3.6178,  -4.8836,   7.6198,   1.7390,  -4.5090,  -2.4024,  -1.2457,
          1.2196,   2.0870,  -1.7720,  -4.0782,   0.1138,   1.2703,  -4.2412,
          6.8343,  -3.4708,  -0.7336,  -0.4390,   6.4675,  -2.2946,   1.0345,
        -10.1635,   3.8853,  -2.6706,   1.0581,  -2.9677,  -0.3949,  -1.0373,
         -4.0974,  -1.9445,   5.0600,   4.7794,  -1.1715,  -3.7145,  -1.8040,
         -6.5361,  -0.5587,   5.2417,   5.4849,  -5.1178,   0.8405,   2.3234,
         -4.8168,   1.2990,  -3.9307,  -0.7040,  -2.5704,   8.3967,  -6.2860,
          4.1478,  -3.1245,   0.3609,   0.4262,   0.5355,   2.6279,  -6.7804,
          2.8876,  -2.6193,   3.4595,  -2.9102,   4.1823,   2.1222,   2.1936,
          4.8309,   2.5502,   2.6925,   1.3631,  -6.0794,   0.9479,   3.2802,
         -1.7285,  -2.7394,  -5.0884,   6.3933,   2.6080,  -3.4339,   0.3773,
         -2.7374,  -0.3503,   1.0064,   3.7774,  -2.7219,  -1.8260,   2.2916,
          0.7168,  -0.8537,   1.7151,  -3.9340,  -0.7165,  -6.2626,  -1.2006,
         -2.3050,  -1.2800,  -3.8862,  -2.5641,   1.8945,   6.5232,   2.7586,
          0.3335,  -2.7098,   3.4165,   3.2112,   3.4328,  -0.0602,   4.7548,
         -1.0049,  -1.4926,   4.0510,  -2.7445,   1.1378,  -0.6707,  -3.0580,
          6.5504,  -6.2017,  -0.5045,  -2.2625,  -1.4709,   3.3636,  -2.5253,
          1.9836,  -5.7901,  -4.4579,   0.1904,   4.5738,   6.2833,  -2.7035,
          1.9133,  -0.5775,  -1.6240,  -2.5966,   2.6923,  -5.6282,   2.8806,
          1.7681,  -7.4464,   8.8879,   2.2289,  -1.0145,   2.0742,  -1.9692,
         -1.3047,   0.6656,  -3.5828,  -0.0479,  -0.3919,   2.1507,  -2.8251,
         -0.8634,   5.8463,   5.6431,   0.0788,   4.3388,   0.6018,  -3.0708,
          6.4656,  -1.8403,  -2.6984,   5.9702,  -2.1216,  -1.0622, -10.9845,
         -1.1060,  -4.9018,   9.7719,   0.2375,   7.4807,   2.8732,   6.2850,
          2.6425,   0.8915,  -2.8190,  -0.2961,  -2.7627,   2.5180,   0.5886,
         -2.4356,  -9.3012,   0.5746,   1.8409,  -2.6565,   1.3937,   0.5170,
         -0.8063,   3.9137,  -6.4429,   1.9446,  -0.5644,   0.8660,   2.9569,
         -1.6849,   0.6454,   0.8093,  -0.6890,  -1.4631, -11.6540,  -2.8805,
          3.6381,  -1.9918,  -5.0443,  -1.5091,  -7.4885,  -0.8082,  -1.0819,
         -0.9287,   0.9537,   1.5744,  -8.1824,  -1.6132,  -2.4886,  -0.3854,
         -4.0974,  -0.5195,  -4.7686,  -4.3962,   1.2075,   0.1470,   1.0125,
         -2.9973,   0.1969,  -4.4334,  -1.5233,  -6.3353,  -0.0447,  -0.2297,
          1.1881,  -5.3978,  -3.8309,  -0.2090,   1.4422,   0.0378,   4.4332,
          4.8856,   2.2798,  -1.8138,   1.8378,  -1.4327,  -3.0749,  -1.5448,
         -5.0381,  -1.1123,   0.9725,   1.1313])
Layer 0 - Batch Norm Var: tensor([ 2.2129,  4.2581,  1.5645,  1.1479,  2.7509,  1.2248,  1.2015,  2.0095,
         0.4783,  1.4270,  0.7131,  1.6250,  2.1954,  1.0315,  1.6389,  2.2933,
         4.2575,  1.1566,  2.5708,  1.0358,  1.4173,  1.1410,  1.0916,  0.8749,
         2.4558,  0.9886,  1.6295,  2.5855,  3.8495,  1.6077,  1.3385,  0.5990,
         3.2687,  1.7442,  0.8982,  6.5435,  2.3786,  0.9989,  0.6917,  1.6008,
         1.1450,  1.0955,  1.8035,  0.5531,  4.4829,  2.2816,  0.8355,  1.8992,
         0.7887,  5.1386,  0.5897,  2.8534,  4.1798,  3.5106,  1.0850,  1.5236,
         1.7866,  0.8540,  1.5860,  1.2096,  1.0579,  4.0113,  3.0537,  1.7031,
         2.8021,  0.8876,  0.6428,  1.5763,  2.9225,  4.6388,  1.3535,  1.2284,
         1.9681,  3.0221,  2.1908,  1.7804,  1.7837,  3.5713,  1.6001,  2.8674,
         2.1260,  3.2417,  1.1730,  2.0611,  1.5082,  1.6231,  2.3473,  4.7399,
         1.0765,  1.5725,  1.0578,  1.0920,  0.9509,  1.3869,  1.7439,  1.7448,
         1.2775,  0.9664,  1.4459,  0.9477,  3.0648,  3.6398,  1.1896,  2.0251,
         0.9473,  2.1759,  0.7129,  1.5837,  1.5447,  0.9212,  4.5148,  1.0786,
         0.9177,  1.0355,  2.5437,  2.5775,  2.2503,  0.8674,  2.5211,  1.0744,
         1.3269,  3.3965,  1.2463,  0.7458,  0.9633,  1.3211,  4.7716,  3.8100,
         0.9012,  0.6970,  0.9653,  2.3671,  1.3652,  1.2286,  2.0394,  1.7455,
         1.1903,  2.5884,  3.0803,  1.2519,  1.1327,  0.9127,  1.4102,  1.1614,
         4.3690,  2.6536,  1.2594,  0.9539,  4.1625,  5.5602,  1.4609,  1.0786,
         1.2386,  1.2737,  1.4361,  1.0605,  2.2583,  1.0340,  0.7889,  1.3294,
         2.0952,  0.8444,  3.1228,  3.0381,  0.7053,  2.0662,  0.8168,  1.5208,
         2.5181,  1.0638,  1.3167,  3.9599,  0.8969,  1.3633,  8.1828,  0.8203,
         2.1924,  6.6842,  1.2384,  4.7070,  1.7959,  4.0146,  1.4882,  1.4157,
         1.5657,  0.6996,  2.5096,  1.1255,  1.6546,  1.8374,  6.6510,  0.8175,
         1.9467,  1.4687,  0.5770,  0.7613,  2.0176,  2.2845,  3.7633,  1.6344,
         0.7325,  1.0867,  2.4960,  1.6158,  1.0920,  1.9401,  0.9629,  2.1385,
        10.5539,  0.8922,  1.9327,  1.9306,  1.9158,  1.0514,  3.5730,  1.3178,
         1.4328,  1.7259,  2.2587,  0.8550,  4.1891,  0.6325,  1.5681,  1.3613,
         2.0463,  1.0246,  2.1060,  2.6432,  1.9231,  0.5558,  1.4547,  1.8685,
         0.7399,  2.9042,  1.1552,  3.6003,  1.5588,  0.8042,  1.0215,  3.0661,
         2.0132,  0.9094,  0.7996,  1.7638,  3.0124,  2.7530,  0.8447,  1.4584,
         0.8485,  1.2813,  1.9437,  0.8913,  2.2664,  1.1860,  0.8598,  1.3022])
Layer 1 - Weight Sample: tensor([[-0.0157,  0.0213,  0.1257,  ..., -0.0990,  0.0359,  0.0221],
        [ 0.0337, -0.0116, -0.0155,  ...,  0.0730, -0.0398,  0.0195],
        [ 0.0033,  0.0962,  0.0020,  ..., -0.1436,  0.0683, -0.0269],
        [ 0.0230, -0.0332, -0.0440,  ...,  0.0078,  0.1226, -0.0221],
        [-0.0029, -0.0155, -0.0527,  ..., -0.1205, -0.0472, -0.0233]])
Layer 1 - Batch Norm Mean: tensor([-1.1906e-02,  2.4036e-02, -1.2882e-02, -1.4622e-02, -1.1379e-02,
        -3.4075e-02, -2.4948e-02, -3.1343e-02, -3.6886e-03, -2.0119e-02,
         2.3910e-02,  2.8157e-02,  1.4139e-02, -3.6669e-04, -1.2674e-03,
         6.1407e-03, -2.4216e-02,  1.1598e-02,  2.0475e-05, -2.5373e-02,
        -3.9517e-02,  2.3761e-02,  1.8763e-03, -1.7401e-03,  2.3260e-02,
         3.1077e-02, -1.9701e-02,  1.9209e-03,  2.1302e-02,  3.8997e-02,
         1.5342e-02, -6.1390e-03, -7.5374e-03, -3.6400e-03, -3.9551e-02,
         3.2559e-02, -2.8934e-02, -2.1317e-02, -2.0187e-02,  7.9087e-03,
         2.9675e-02,  2.8529e-03,  1.0620e-02,  3.1562e-02, -4.0716e-04,
         3.2024e-02,  6.0942e-02,  1.1232e-03,  1.6113e-02,  3.9323e-03,
         1.5663e-03,  4.5959e-02,  1.5273e-02,  6.9699e-03,  2.1399e-02,
        -1.7561e-02,  1.0804e-02, -2.1370e-03,  1.5806e-02, -1.6259e-02,
         4.9098e-03,  2.9092e-02,  5.1611e-02, -4.7989e-02,  1.9746e-03,
         1.6125e-02, -1.6474e-02,  1.7392e-02,  1.4701e-02,  1.1586e-02,
         3.9907e-02,  1.8079e-02,  2.9370e-02, -5.6068e-04, -8.9431e-04,
        -1.4111e-02,  3.8027e-03, -3.1256e-03,  1.1224e-02, -1.4621e-02,
        -3.1592e-02,  6.4182e-03,  6.7913e-04,  6.5428e-03,  8.0905e-04,
         1.9925e-02,  4.4682e-03, -4.1484e-02, -2.0988e-02,  4.8619e-02,
        -5.2265e-02, -1.6137e-02, -1.5404e-02, -4.9292e-05, -2.3277e-02,
        -7.6301e-03, -2.4857e-02, -4.3137e-02,  2.6080e-02,  3.7195e-02,
        -1.9286e-02, -4.7563e-02,  2.4085e-02, -2.1008e-02, -1.5490e-02,
         5.9102e-03,  9.7877e-03,  1.0924e-03, -1.8753e-02,  7.2809e-03,
        -2.3883e-02, -1.7436e-02, -7.9046e-03,  4.1053e-02, -1.2479e-02,
         5.9419e-03,  9.0342e-03, -8.8626e-03, -2.8848e-02, -2.5744e-02,
        -3.3211e-02, -2.1842e-02,  2.7052e-02,  2.7028e-02, -2.4048e-02,
        -1.8124e-03,  2.0701e-02,  4.6484e-03,  2.3968e-04, -7.7031e-03,
        -1.3746e-02, -1.5837e-02,  2.3966e-02,  4.4484e-02, -1.8217e-02,
         9.3969e-03,  3.6542e-02, -1.9542e-02,  5.7077e-03,  1.9034e-02,
         1.9846e-02,  3.1376e-02,  3.2977e-03, -1.4267e-02,  1.2334e-02,
        -1.6784e-02, -3.8706e-03,  9.9986e-03,  2.5207e-02, -1.8719e-02,
        -1.2408e-02, -2.5316e-02, -5.4532e-02, -1.7184e-02, -1.7810e-02,
         1.4236e-03,  2.2920e-02, -1.8235e-02,  3.2182e-02, -3.7238e-02,
         2.1324e-03,  1.2154e-02, -6.7782e-03,  9.3125e-03, -2.3549e-02,
        -3.4949e-02, -2.4520e-02,  1.3146e-03, -5.3403e-03, -3.5476e-03,
         2.0258e-02, -8.6649e-03, -5.6897e-03,  2.3174e-02,  4.8893e-02,
         6.4650e-03,  1.3292e-02,  6.6687e-03, -2.3335e-03, -1.1840e-02,
        -1.6543e-02, -5.1383e-03,  6.8061e-02,  7.4385e-03, -4.0569e-02,
        -1.0361e-02, -1.6412e-02, -1.4717e-02, -1.7676e-02, -4.0641e-02,
         1.1496e-02,  7.3105e-03, -1.4161e-02, -3.1445e-02, -4.5986e-02,
        -2.8331e-02, -5.0770e-03,  9.5680e-04, -4.4204e-02,  1.2096e-02,
        -2.2998e-02,  1.2955e-02,  3.3538e-02,  1.5617e-02, -2.0167e-02,
         2.2202e-02,  2.6871e-02, -1.2323e-02,  4.2490e-03,  3.3806e-02,
         1.3121e-02, -2.5730e-02, -9.9550e-04,  1.5833e-02, -5.6279e-02,
         7.4607e-03, -4.0813e-02,  2.6526e-02, -5.4154e-02, -3.5428e-02,
         1.4094e-02,  1.5590e-02, -1.0572e-02,  5.9854e-03,  3.3140e-03,
        -6.3186e-02,  1.6969e-02, -4.6734e-03,  2.9704e-02, -2.4942e-02,
         4.1321e-03,  1.0481e-02,  2.0707e-02, -2.2853e-02, -2.5519e-02,
        -1.1837e-02, -1.0620e-02,  1.7712e-02,  2.6878e-02, -2.3527e-02,
        -1.4540e-03,  6.7841e-03,  3.3665e-02,  6.1958e-03,  1.1016e-02,
         9.1850e-03,  3.0719e-02, -1.1770e-02,  1.5821e-02, -9.3853e-03,
         7.7342e-03, -1.7323e-02, -1.7181e-02,  1.0442e-02, -7.8576e-03,
         8.4114e-03])
Layer 1 - Batch Norm Var: tensor([0.4173, 0.7729, 0.8685, 0.3195, 0.5136, 0.1962, 0.4155, 0.8980, 0.2937,
        0.5150, 0.3871, 0.7555, 0.7411, 0.4253, 0.8696, 0.5021, 0.3970, 0.4208,
        0.3445, 0.2341, 0.7603, 0.2897, 0.2684, 0.2867, 0.4636, 0.3771, 0.4701,
        0.3338, 0.2511, 0.7589, 0.4408, 0.4302, 0.3562, 0.2784, 0.4291, 0.4250,
        0.6734, 0.3791, 0.3203, 0.5257, 0.8938, 0.6065, 0.4889, 0.4050, 0.3367,
        0.4959, 0.4387, 0.2612, 0.4115, 0.3318, 0.4631, 0.6450, 0.4656, 0.4981,
        0.3184, 0.3781, 0.4241, 0.5654, 0.6626, 0.2761, 0.2387, 0.5455, 0.5086,
        1.3460, 0.2237, 0.5560, 0.6153, 0.2213, 0.4738, 0.3700, 0.4641, 0.3886,
        0.2887, 0.4165, 0.4211, 0.4003, 0.3278, 0.2003, 0.4424, 0.6437, 0.6495,
        0.3420, 0.2320, 0.2276, 0.4299, 0.3332, 0.7946, 0.5752, 0.6106, 0.3987,
        0.4065, 0.2607, 0.5367, 0.4812, 0.6238, 0.2598, 0.2817, 0.6137, 0.5226,
        0.5567, 0.3988, 1.3853, 0.8445, 0.5375, 0.3271, 0.4755, 0.4485, 0.6799,
        0.2734, 0.2696, 0.4690, 0.2658, 0.5132, 0.8761, 0.6866, 0.4588, 0.2753,
        0.4159, 0.2620, 0.6459, 0.4102, 0.4185, 0.3073, 0.3449, 0.2723, 0.4008,
        0.3268, 0.3102, 0.3606, 0.4965, 0.7971, 0.3144, 0.5423, 0.4267, 0.3212,
        0.3232, 0.8320, 0.7537, 0.3338, 0.3296, 0.5143, 0.2626, 0.6578, 0.4503,
        0.5301, 0.4977, 0.6698, 0.5945, 0.6227, 0.3430, 0.3740, 0.4660, 0.7730,
        0.6274, 0.4663, 0.4663, 0.5053, 0.4514, 0.7238, 0.5218, 0.4182, 0.3653,
        0.3514, 0.3858, 0.4708, 0.3291, 0.3619, 0.2794, 0.5746, 0.2268, 0.6042,
        0.3505, 0.2054, 0.8408, 0.6912, 0.9788, 0.6044, 0.3097, 1.2544, 0.2774,
        1.0771, 0.3204, 0.7374, 0.2513, 0.3651, 0.4199, 0.4998, 0.3331, 0.3569,
        0.6096, 0.4715, 0.2349, 0.2636, 0.2536, 0.5109, 0.3403, 0.3608, 0.4401,
        0.5366, 0.3147, 0.3421, 0.5151, 0.5689, 0.5515, 0.2799, 0.6418, 0.5735,
        0.3951, 0.3747, 0.5192, 0.3465, 0.1727, 0.2028, 0.3361, 0.4980, 0.3211,
        1.0016, 0.4582, 1.1510, 0.3986, 0.4107, 0.8750, 0.2684, 0.3843, 0.3995,
        0.5497, 0.3176, 0.3100, 0.9949, 0.3486, 0.5773, 0.3086, 0.5468, 0.5615,
        0.7100, 0.2080, 0.3070, 0.1993, 0.3244, 0.5330, 0.3940, 0.3778, 0.3002,
        0.2817, 0.3069, 0.2515, 0.6525, 0.3671, 0.5846, 0.4469, 0.5486, 0.4761,
        0.6612, 0.2210, 0.4866, 0.6313])
Layer 2 - Weight Sample: tensor([[ 0.0734, -0.0317,  0.0099,  ...,  0.0138,  0.0591,  0.1394],
        [-0.0200,  0.1405,  0.0492,  ..., -0.0703, -0.0347,  0.0400],
        [ 0.0120,  0.0392,  0.0464,  ..., -0.0585, -0.0002,  0.0602],
        [ 0.0364,  0.0675, -0.0864,  ...,  0.0300, -0.0288,  0.0383],
        [ 0.0132,  0.0421, -0.0308,  ..., -0.0463,  0.0426,  0.0144]])
Layer 2 - Batch Norm Mean: tensor([-0.0203, -0.0031,  0.0251,  0.0025, -0.0146,  0.0138,  0.0524,  0.0011,
        -0.0156, -0.0102,  0.0049,  0.0092,  0.0023,  0.0083, -0.0094, -0.0002,
         0.0313, -0.0172,  0.0231,  0.0004, -0.0118,  0.0060,  0.0222,  0.0103,
         0.0032,  0.0262,  0.0209, -0.0120, -0.0027, -0.0124,  0.0265, -0.0046,
        -0.0096, -0.0040, -0.0283,  0.0086,  0.0279, -0.0057,  0.0008, -0.0073,
        -0.0068, -0.0188, -0.0175,  0.0149, -0.0174,  0.0193, -0.0289,  0.0110,
        -0.0011,  0.0044, -0.0032, -0.0031,  0.0322,  0.0074,  0.0126,  0.0099,
         0.0090, -0.0109,  0.0255, -0.0286, -0.0215,  0.0121, -0.0020,  0.0169,
        -0.0026, -0.0227,  0.0293, -0.0195, -0.0112,  0.0064,  0.0181,  0.0161,
         0.0009, -0.0336, -0.0174, -0.0055, -0.0116, -0.0089,  0.0156,  0.0034,
        -0.0204,  0.0173,  0.0027,  0.0034,  0.0148, -0.0073,  0.0157,  0.0056,
        -0.0088, -0.0099, -0.0186,  0.0069,  0.0076,  0.0065,  0.0487, -0.0039,
        -0.0173,  0.0175, -0.0018,  0.0288,  0.0113,  0.0167,  0.0019, -0.0311,
         0.0149, -0.0260,  0.0175,  0.0139,  0.0145,  0.0092, -0.0319,  0.0217,
        -0.0381, -0.0241,  0.0429, -0.0015,  0.0071,  0.0113,  0.0063, -0.0027,
         0.0133, -0.0221, -0.0194, -0.0004, -0.0076, -0.0076, -0.0120,  0.0119,
         0.0040,  0.0186, -0.0069, -0.0215, -0.0042, -0.0061, -0.0293, -0.0150,
        -0.0061,  0.0089,  0.0108, -0.0153,  0.0231,  0.0075, -0.0120,  0.0102,
        -0.0025, -0.0046,  0.0056, -0.0255,  0.0194, -0.0201,  0.0300,  0.0036,
        -0.0326, -0.0153,  0.0049,  0.0049, -0.0049,  0.0170,  0.0056, -0.0050,
         0.0224, -0.0308, -0.0219, -0.0062,  0.0009, -0.0305, -0.0033, -0.0162,
        -0.0292,  0.0083,  0.0059, -0.0129,  0.0163,  0.0096, -0.0059,  0.0144,
        -0.0018,  0.0176,  0.0135, -0.0158, -0.0025,  0.0057, -0.0150,  0.0132,
        -0.0149,  0.0038,  0.0206, -0.0319, -0.0304, -0.0002, -0.0156, -0.0118,
         0.0141, -0.0217,  0.0049, -0.0116, -0.0258, -0.0107, -0.0147,  0.0280,
         0.0248,  0.0038, -0.0495, -0.0103,  0.0522,  0.0061, -0.0014,  0.0082,
        -0.0008,  0.0005,  0.0013,  0.0159, -0.0119, -0.0184,  0.0091,  0.0324,
         0.0193, -0.0169, -0.0103,  0.0014, -0.0045, -0.0056, -0.0445, -0.0038,
        -0.0251,  0.0026,  0.0477, -0.0205, -0.0168, -0.0214, -0.0282,  0.0168,
        -0.0021, -0.0137,  0.0147, -0.0289, -0.0227, -0.0237,  0.0316, -0.0148,
        -0.0160,  0.0135, -0.0399, -0.0260, -0.0179,  0.0378,  0.0032, -0.0236,
         0.0298, -0.0103,  0.0206,  0.0044, -0.0335, -0.0492,  0.0097, -0.0274])
Layer 2 - Batch Norm Var: tensor([0.4243, 0.4716, 0.5655, 1.2592, 0.6792, 0.2794, 1.6216, 1.2079, 1.0548,
        1.1512, 0.8652, 0.4957, 0.3814, 0.2735, 0.4142, 0.9930, 0.4310, 0.2947,
        0.3016, 0.4947, 0.4575, 0.5765, 0.5640, 0.3283, 0.3724, 0.6242, 0.3708,
        0.7796, 0.6842, 0.5540, 0.4111, 0.4765, 1.2747, 0.5628, 0.4441, 0.3862,
        0.8457, 0.5506, 0.3255, 0.3577, 0.3734, 0.4615, 0.7016, 0.3568, 0.3489,
        1.4253, 0.9922, 0.8074, 0.7063, 0.4050, 0.4180, 0.3977, 0.5770, 0.5285,
        0.7776, 0.4493, 0.4126, 0.4558, 0.6916, 0.4864, 1.2713, 0.2134, 0.3617,
        0.9420, 0.4709, 0.8971, 1.4613, 0.2691, 0.6205, 0.2881, 0.6408, 0.7091,
        2.0878, 0.4833, 0.5794, 0.3726, 0.2795, 0.8188, 1.2979, 0.4455, 0.4603,
        0.7685, 0.2567, 0.4883, 0.4157, 0.6106, 0.6496, 0.2469, 0.4062, 0.4859,
        0.3244, 0.7183, 0.3277, 0.3903, 0.7685, 0.4493, 0.7620, 0.2957, 0.6110,
        1.1922, 0.5280, 0.4094, 0.2642, 0.9349, 0.4707, 0.6042, 0.4667, 1.6148,
        0.4102, 0.3682, 0.3611, 0.8886, 0.6267, 1.0029, 0.4127, 0.7223, 0.6040,
        0.3330, 0.3853, 0.6109, 0.7332, 1.4643, 0.5814, 0.5697, 0.2937, 0.5486,
        1.6854, 0.7705, 0.2964, 0.5988, 0.2994, 0.3776, 0.5529, 0.5097, 1.1310,
        0.4853, 1.0953, 0.3290, 0.6597, 0.7192, 1.0526, 0.4894, 0.3036, 0.2734,
        0.7345, 0.2655, 0.7386, 0.4444, 0.6545, 0.2976, 0.5281, 0.2387, 0.8963,
        0.5857, 0.2754, 0.2560, 0.2738, 0.3996, 0.4721, 0.6346, 0.3863, 0.4935,
        1.1966, 0.6215, 0.9126, 0.8728, 0.5778, 0.5816, 0.4275, 0.2580, 0.3388,
        0.5937, 0.6635, 0.6505, 0.3712, 0.4354, 0.4186, 0.7973, 0.6953, 0.6566,
        0.4799, 0.6564, 0.4131, 0.2644, 0.2870, 0.4822, 0.8128, 0.4270, 1.1692,
        0.8748, 0.5664, 0.5355, 0.6005, 1.0040, 0.4408, 0.2927, 0.2324, 0.6062,
        0.6821, 0.6478, 0.3496, 0.9082, 0.6335, 0.8625, 0.8743, 1.1027, 0.3174,
        0.4280, 0.7716, 0.2748, 0.3651, 0.6070, 0.4716, 0.5076, 1.1939, 0.2550,
        0.2687, 0.7614, 0.2612, 0.5038, 0.7173, 0.9275, 0.3201, 0.4987, 0.8378,
        0.5258, 1.3129, 0.3332, 0.6085, 0.4794, 0.3925, 0.5164, 0.6683, 0.8380,
        0.2701, 0.8813, 0.4737, 0.6373, 0.5605, 0.5164, 0.7801, 0.5649, 1.1873,
        0.2743, 0.2837, 0.3246, 0.3148, 0.4758, 0.3841, 0.5565, 1.3212, 0.4422,
        0.5356, 1.1067, 0.9532, 0.8679])
Layer 3 - Weight Sample: tensor([[-0.0768, -0.0031,  0.0326,  ...,  0.0438,  0.0437, -0.0356],
        [ 0.1104,  0.0633, -0.1042,  ...,  0.0085,  0.0372, -0.0072],
        [-0.0613, -0.0390, -0.0565,  ...,  0.0200, -0.0789,  0.0753],
        [ 0.1402, -0.0325,  0.0213,  ..., -0.0010, -0.0995, -0.0079],
        [-0.0224, -0.0822, -0.0137,  ..., -0.0186, -0.0758, -0.0756]])
Layer 3 - Batch Norm Mean: tensor([-6.0301e-03,  1.8148e-02,  9.3197e-03, -8.0104e-03,  9.1812e-03,
        -3.6468e-03, -1.4507e-02,  1.4473e-02, -6.6351e-03,  5.0908e-04,
        -8.8403e-03,  1.5115e-02,  1.4900e-02,  1.1739e-03,  1.6657e-02,
        -5.5198e-03, -1.8167e-03, -2.6849e-02, -2.5215e-02,  2.6256e-03,
        -1.8830e-02, -3.1273e-02, -7.5732e-03,  9.2659e-03,  2.6149e-02,
         9.7846e-03, -2.7459e-02,  1.1059e-02, -7.7679e-03, -3.6881e-03,
         2.1944e-02,  2.4569e-02, -2.1829e-02, -1.3110e-02, -1.9742e-03,
        -1.1441e-02, -1.4298e-02, -1.2258e-02,  7.1706e-03,  2.1740e-02,
         1.7426e-02, -7.2189e-03, -1.2189e-03,  2.4134e-02, -1.2215e-03,
        -1.1754e-02,  1.3411e-02,  2.0232e-02, -5.5143e-03, -2.9790e-02,
        -4.1525e-03,  1.0923e-02,  3.0405e-02,  2.6621e-04,  1.7155e-03,
        -8.5551e-03,  6.8330e-03,  4.5421e-03, -3.2853e-03,  2.8812e-02,
        -4.4914e-03,  2.5957e-02, -1.3280e-02,  1.5046e-03, -2.8373e-02,
         1.8289e-02,  3.2972e-02,  9.6882e-03,  1.9146e-02,  1.8780e-02,
         9.3828e-03, -1.3575e-03, -4.4792e-03, -2.8337e-02, -2.8819e-03,
         2.6010e-02, -8.6913e-03,  4.6335e-03,  2.2501e-02, -2.2813e-02,
         1.1470e-02, -3.3734e-02,  5.2891e-03,  5.9317e-03,  9.9676e-03,
         1.0085e-02, -1.3871e-03,  9.1184e-04,  6.6568e-03,  4.4807e-02,
        -1.9758e-02,  6.0030e-04,  4.7325e-04,  2.4665e-02,  1.0330e-02,
        -8.1197e-03,  2.8325e-02, -2.8714e-03,  2.6179e-02,  1.1848e-02,
        -7.5510e-03, -7.9404e-04, -1.9888e-02, -1.5728e-02, -7.5289e-03,
        -1.0752e-02, -4.3896e-03,  4.3870e-03,  2.5313e-02, -3.1543e-03,
         6.8353e-03, -1.5707e-02,  9.6810e-06,  5.0895e-03, -3.3115e-03,
        -7.0055e-03, -1.8408e-02,  6.4906e-04, -2.1343e-03,  1.4996e-02,
         8.4472e-03,  1.3880e-02,  2.0628e-02,  1.3854e-02, -4.4451e-03,
        -1.6356e-02,  1.4294e-02,  5.1773e-03,  1.2147e-02, -2.3481e-02,
         1.3365e-02,  2.9284e-02,  9.3654e-03, -2.8624e-02, -2.5839e-04,
         5.0230e-03,  2.4013e-02, -4.3039e-04, -3.1991e-02,  1.3750e-02,
         2.4519e-03,  1.0632e-02, -1.7055e-02, -1.9543e-02,  8.7991e-03,
         4.9717e-03,  3.6648e-02,  3.6009e-04, -3.3516e-02, -1.1382e-02,
        -1.5075e-02, -1.6247e-03,  3.4254e-03,  1.6842e-02, -2.3376e-03,
        -1.5477e-02,  7.0278e-03, -2.6381e-02,  1.5315e-03, -1.5950e-02,
         2.0543e-02,  1.0362e-02, -1.0358e-03, -6.9241e-03,  2.0983e-02,
         1.6206e-02, -1.0195e-02, -2.5110e-02, -2.2580e-02, -3.0183e-02,
         3.0773e-02, -4.4870e-03, -3.9879e-02, -7.7347e-03, -4.1481e-03,
         1.2210e-03, -6.6422e-03,  3.3965e-03,  7.7636e-03,  1.0185e-03,
        -1.2325e-02, -2.0841e-02, -3.3803e-02, -1.5129e-03,  1.2736e-02,
         9.9516e-04, -3.3387e-02, -1.0268e-03, -1.4542e-02, -4.6220e-03,
        -2.6239e-02,  2.5953e-02,  1.7805e-02,  1.5647e-02,  8.5537e-03,
        -5.1050e-03, -7.9999e-03, -3.3503e-03, -1.0652e-02,  1.3401e-02,
         4.6764e-04,  4.8554e-03, -1.0488e-02,  3.0045e-03,  9.6052e-03,
         1.1190e-02,  5.8243e-03, -4.3006e-03,  7.5901e-04, -5.1323e-03,
        -1.6005e-02,  7.0605e-03,  2.6978e-02,  2.1996e-02,  1.9659e-03,
         1.4572e-02,  2.6241e-03,  1.1026e-02, -8.7940e-03, -4.7692e-03,
        -7.4068e-03, -7.4674e-03, -6.1239e-03, -2.4907e-03,  2.5242e-04,
         6.3074e-03,  1.5183e-02,  1.9596e-02, -3.3175e-03, -1.5693e-02,
         3.6822e-03, -2.9626e-02, -1.1577e-02, -1.4770e-02, -9.9190e-03,
        -1.2918e-03,  1.8521e-03,  5.7096e-03,  6.9816e-03, -1.3299e-02,
        -2.4724e-02,  1.4766e-02, -2.7571e-02, -1.1089e-02, -3.1366e-02,
         1.5378e-02,  4.5964e-03, -9.2406e-03, -2.4185e-02, -4.3874e-02,
        -1.5678e-02,  5.9355e-03,  1.2162e-02, -9.6794e-03, -1.2774e-02,
         5.1006e-02])
Layer 3 - Batch Norm Var: tensor([0.4170, 0.2411, 0.4797, 1.9278, 0.2967, 0.4273, 0.2889, 0.7002, 0.4981,
        1.2058, 0.1416, 0.3672, 0.6924, 0.8980, 0.7240, 0.2676, 0.9625, 0.1765,
        0.5415, 0.4666, 0.3370, 0.4355, 1.1449, 0.4049, 0.3222, 0.3674, 0.3917,
        0.2948, 0.2147, 0.4432, 0.3507, 0.3351, 1.7960, 0.3285, 0.4642, 0.8856,
        0.3579, 0.5367, 0.6388, 2.1081, 0.5094, 0.9099, 0.3857, 0.6140, 0.4887,
        0.4359, 0.3942, 0.4594, 0.4390, 1.0467, 0.5265, 0.5321, 1.0740, 0.2061,
        0.5742, 0.6288, 0.3972, 0.2188, 0.3883, 1.5137, 0.4025, 0.5069, 1.3997,
        0.7331, 0.8418, 0.4248, 1.5328, 0.4005, 0.7664, 0.3484, 0.9511, 0.3430,
        0.7473, 0.5482, 0.4162, 0.8567, 0.6846, 0.7354, 0.4326, 0.3522, 0.4060,
        0.8016, 0.3802, 1.3002, 1.2346, 0.3367, 0.2832, 0.2453, 0.3064, 0.9749,
        1.6307, 0.3938, 0.2923, 1.1703, 0.2543, 0.3341, 1.2675, 0.4592, 4.6014,
        1.0024, 0.4067, 0.6297, 0.4212, 0.7769, 0.5115, 1.0301, 0.6718, 1.2521,
        0.2494, 0.3502, 0.8224, 0.3901, 0.4059, 0.4395, 0.2129, 1.6653, 0.5220,
        0.7505, 0.7600, 0.4184, 0.8206, 0.3757, 0.3951, 0.8080, 0.4622, 0.2562,
        1.5225, 0.4573, 0.7628, 0.9287, 0.5333, 0.5659, 1.1238, 0.3425, 0.4586,
        0.6582, 0.4673, 0.2592, 0.4611, 0.3218, 0.5863, 0.4259, 0.4897, 0.6184,
        0.3196, 0.3515, 0.5267, 0.2174, 0.6043, 0.7391, 0.5653, 0.2154, 0.4315,
        0.5700, 0.3379, 0.6567, 0.4432, 1.4532, 0.3198, 0.6683, 1.1952, 0.9714,
        0.8488, 0.7245, 0.2907, 0.5792, 0.8008, 1.2396, 0.7662, 0.2242, 1.1045,
        0.4224, 0.4285, 0.6345, 0.2610, 0.6672, 0.3587, 0.4417, 1.4098, 0.2946,
        0.5024, 0.2730, 0.9429, 0.6190, 0.4300, 1.4259, 0.3040, 0.4985, 1.0816,
        0.7314, 0.6929, 0.2282, 0.4977, 0.3635, 0.9117, 0.5259, 0.6687, 0.6507,
        0.9121, 0.6609, 0.5959, 1.1230, 0.3522, 0.6555, 0.7924, 2.4580, 0.4817,
        0.9539, 0.4332, 0.5781, 0.3741, 0.5321, 1.1365, 0.8334, 0.7821, 0.7344,
        0.8368, 0.3515, 0.4624, 0.4645, 0.2986, 0.4138, 0.3052, 0.5738, 1.0066,
        0.3309, 0.4375, 0.6767, 0.4959, 0.4262, 0.4035, 0.9564, 0.8981, 0.3964,
        0.5689, 0.4339, 0.4408, 0.3041, 0.4410, 0.9429, 0.5715, 0.7359, 1.0716,
        0.5211, 0.4032, 0.5298, 0.7519, 0.3802, 0.5594, 0.2481, 0.3270, 0.3392,
        0.4466, 0.2293, 0.2635, 2.6450])
Layer 4 - Weight Sample: tensor([[ 2.5232e-02, -4.9914e-02,  3.5325e-02,  ..., -8.7765e-02,
         -3.2882e-02,  2.7278e-02],
        [-5.3529e-02,  4.4917e-02, -9.5233e-02,  ..., -4.1839e-02,
          1.2944e-01,  7.4616e-02],
        [ 9.1512e-02,  6.0847e-02, -1.0190e-01,  ...,  1.8157e-02,
         -2.6754e-02, -6.9080e-02],
        [-8.9643e-02,  2.7922e-02,  1.3136e-01,  ..., -1.5668e-03,
          4.8773e-02, -1.5165e-02],
        [ 4.8843e-05,  4.9732e-02, -8.1572e-02,  ..., -9.3337e-03,
         -2.4631e-02,  3.0138e-02]])
Layer 4 - Batch Norm Mean: tensor([-3.2217e-02, -2.4468e-02, -1.8505e-02, -3.3825e-02,  4.1612e-02,
        -1.3876e-02, -3.8686e-03,  5.4425e-03,  2.3070e-02, -1.1545e-02,
         6.4277e-03,  1.0282e-02, -1.2567e-02, -1.5414e-02, -1.2980e-02,
         2.3523e-02, -1.6077e-02,  1.3323e-02, -1.0231e-02,  1.4061e-02,
         7.8375e-03,  1.3380e-02,  1.3975e-02,  1.7235e-02, -1.7126e-02,
        -7.1678e-03,  2.1000e-02,  2.4985e-02, -2.1098e-03, -1.2403e-02,
        -2.4459e-02, -1.6477e-02,  1.1572e-02,  4.0046e-03, -1.0647e-02,
         2.1064e-02, -4.8303e-02, -8.1268e-03, -2.5667e-02,  3.9555e-03,
         2.2524e-02, -1.4166e-03,  2.5846e-02,  8.5348e-03,  1.3983e-02,
        -2.4906e-04,  8.8371e-03,  1.8176e-02,  7.1400e-03,  2.4319e-03,
        -8.5268e-03, -2.5847e-02,  1.7047e-03,  3.1107e-03,  4.0641e-03,
         1.3551e-02, -3.5056e-02,  1.5023e-02, -4.4573e-02,  1.9788e-05,
         1.0782e-02,  1.5423e-02,  1.1284e-02, -4.0043e-03, -1.1590e-02,
         3.2146e-02,  4.7284e-03, -1.1257e-02, -9.4744e-03,  2.2854e-02,
         1.5367e-03,  2.8101e-03, -1.6654e-02, -3.1502e-02, -5.3220e-03,
         1.1993e-02,  1.2988e-02, -1.5775e-02, -1.7642e-02, -1.8566e-02,
        -1.7657e-02, -3.4080e-02,  6.6373e-03, -2.9307e-02, -2.7348e-02,
        -1.5563e-02, -2.0031e-02,  4.0386e-03,  1.3742e-03,  2.6733e-02,
         7.2232e-03, -2.1331e-02, -3.5743e-02,  1.3841e-02,  9.9704e-03,
         1.9035e-02,  7.0860e-03, -2.4134e-02,  3.2880e-02, -1.5700e-04,
         3.1220e-02, -1.4084e-02,  5.4550e-03,  2.3007e-02,  1.1241e-03,
        -1.6745e-04, -1.8289e-02, -3.0306e-03, -1.8204e-02, -2.4592e-02,
         1.3224e-02, -3.2422e-03,  1.9193e-02,  2.1915e-02, -4.9225e-03,
        -2.1996e-02, -1.3424e-02, -2.0444e-02, -8.0102e-03, -6.0128e-03,
         7.5625e-04, -1.0721e-02, -3.1848e-02,  3.4655e-03, -4.3037e-03,
        -1.0130e-02,  4.1193e-03, -2.0817e-02,  2.2352e-02, -2.3135e-04,
        -1.8973e-02,  2.7552e-02,  2.2358e-02,  4.1757e-02, -1.4010e-02,
        -6.9730e-03, -1.1271e-02, -2.1131e-02, -2.3347e-02, -5.7079e-03,
         2.6544e-02,  1.4580e-02, -1.7548e-02, -1.3881e-02,  2.6113e-02,
        -4.1521e-05, -4.3001e-03,  3.3375e-02,  3.5894e-03, -1.1181e-02,
         7.9085e-03, -5.5365e-03, -2.6975e-02, -1.7112e-03, -2.4770e-02,
        -1.2912e-02, -7.1628e-03, -6.6194e-03,  1.9170e-02,  1.8231e-02,
        -4.6559e-04,  8.2738e-04, -1.8282e-02,  4.1377e-03,  1.4043e-02,
        -6.3526e-03, -2.3967e-02, -1.7190e-03,  8.9056e-03, -1.8365e-02,
         3.3491e-02,  3.2533e-03,  2.0584e-02, -8.5128e-03,  6.4310e-03,
        -2.5022e-02, -1.8051e-02,  4.4762e-03, -1.4362e-02,  3.6022e-03,
        -3.6094e-02, -2.1367e-02, -2.0863e-02,  4.3510e-03,  1.7916e-02,
         1.1390e-02, -1.9164e-02, -1.1240e-02, -4.0456e-03,  1.1567e-02,
        -6.9903e-04, -1.8657e-02,  1.5819e-02, -1.7566e-02,  2.9390e-03,
        -2.3735e-02,  1.6405e-02,  1.9054e-02, -2.5904e-03, -6.0488e-04,
        -2.7177e-02,  1.2912e-02, -1.6336e-02,  1.8596e-02,  1.1654e-02,
         2.8109e-02,  3.9018e-03, -2.1626e-02, -1.2263e-02, -9.3984e-03,
         8.8349e-03, -5.2093e-03, -1.0236e-02, -1.3291e-02, -1.2810e-02,
        -1.9118e-03, -1.5219e-02,  5.4349e-03, -1.1725e-02,  3.2039e-03,
        -4.9721e-04,  8.2589e-03,  9.8113e-03, -3.3186e-02, -1.1694e-03,
         2.5408e-03,  6.0992e-04, -1.8263e-02,  1.3351e-03, -1.3210e-02,
        -7.9892e-04,  1.0246e-05,  1.0251e-03,  1.2889e-02,  9.4599e-03,
        -1.4125e-02,  3.4231e-03, -1.1850e-02,  1.4865e-02,  3.2037e-03,
        -5.0672e-03, -1.1417e-02, -1.4142e-02, -7.8520e-03,  4.5636e-03,
        -4.2587e-02, -2.5547e-02, -1.5080e-02, -1.0554e-02, -2.2804e-02,
         1.0966e-02, -8.6926e-03, -1.9146e-02, -1.1155e-02,  4.4313e-03,
         1.6388e-02])
Layer 4 - Batch Norm Var: tensor([0.6719, 0.4482, 1.4963, 0.4848, 0.5649, 0.2731, 0.4102, 0.4567, 0.3250,
        1.5515, 1.2348, 0.3986, 0.8972, 0.4643, 0.9373, 0.2837, 1.0768, 0.3946,
        1.3696, 0.4275, 0.6725, 0.7346, 0.2268, 0.2074, 0.4305, 0.2177, 0.7960,
        0.6788, 0.5910, 1.0485, 0.5155, 0.2831, 0.8378, 0.2325, 0.4934, 0.1506,
        0.5173, 0.8327, 0.3437, 0.2108, 0.3174, 0.4416, 0.2457, 0.5678, 0.3191,
        0.4940, 0.4487, 0.6474, 0.7546, 0.7205, 0.6894, 0.9678, 1.3201, 0.4496,
        0.4277, 0.4120, 0.4538, 0.8087, 1.0549, 0.2408, 0.3513, 0.7497, 0.4232,
        0.5690, 0.1879, 0.5097, 0.4504, 1.2488, 0.5401, 0.5771, 0.2932, 0.5595,
        0.3176, 0.5595, 0.4550, 0.6237, 0.3535, 0.2601, 0.4591, 0.4042, 0.7133,
        0.5831, 0.4847, 0.2747, 0.6043, 0.2628, 0.4935, 0.8674, 0.6323, 0.8499,
        0.7642, 0.7061, 0.4937, 0.6922, 0.3665, 0.3726, 0.3761, 0.5639, 0.4036,
        1.0914, 0.6111, 0.6714, 0.2056, 0.4830, 0.6063, 1.5013, 1.1549, 0.9377,
        0.7280, 0.4815, 0.3807, 0.3186, 0.3796, 0.5489, 0.4825, 0.9679, 0.5870,
        0.4208, 0.4347, 0.4959, 1.3253, 0.1717, 0.7428, 1.4521, 0.5597, 0.6511,
        1.7720, 1.1327, 1.0205, 0.9661, 0.6729, 0.3484, 0.6524, 0.9665, 0.8349,
        0.6384, 1.4735, 0.4023, 0.4390, 0.8322, 0.5426, 0.4683, 0.4464, 0.5146,
        0.3762, 0.5059, 0.2823, 1.2736, 0.4214, 0.5073, 0.4703, 0.3012, 0.2949,
        0.3159, 0.3720, 0.4486, 0.6231, 0.5272, 1.3532, 0.7873, 0.2640, 0.7541,
        0.8299, 0.5719, 0.3323, 0.4545, 0.2418, 0.3088, 0.6677, 1.3083, 0.7367,
        1.1698, 0.6305, 0.3295, 0.7204, 0.5322, 0.3974, 0.7202, 0.4392, 0.4454,
        0.7476, 0.4282, 0.4978, 0.3545, 1.3628, 0.6280, 0.3089, 0.2308, 0.6224,
        0.7094, 0.8038, 0.3957, 0.4084, 0.6595, 0.4896, 0.4552, 1.3509, 0.5453,
        1.4689, 1.0811, 1.2401, 0.3223, 0.4425, 0.3225, 0.9186, 0.5233, 0.4307,
        0.3570, 0.7353, 0.4319, 0.4170, 1.5546, 0.4377, 0.4092, 0.4090, 0.7106,
        0.4072, 1.2619, 0.9500, 0.5315, 0.4079, 0.6923, 0.7413, 0.6233, 0.7247,
        0.3068, 0.5137, 0.3718, 0.4379, 0.6044, 0.3016, 0.2083, 1.1195, 1.1570,
        0.5324, 0.9767, 1.5776, 0.3578, 0.3018, 0.8346, 0.4141, 0.3212, 0.6124,
        0.3386, 0.7017, 1.2484, 0.3908, 0.2247, 1.7461, 0.3361, 0.3995, 0.7314,
        0.3710, 1.0746, 0.6118, 0.4789])
Layer 5 - Weight Sample: tensor([[-0.0587, -0.0437,  0.0576,  ..., -0.0149, -0.1781,  0.0659],
        [-0.1410, -0.0850, -0.0430,  ..., -0.0787, -0.0020,  0.1215],
        [ 0.1773, -0.0193,  0.0592,  ..., -0.0207,  0.0458, -0.0275],
        [-0.0723,  0.2652,  0.1647,  ...,  0.0852, -0.0358, -0.1835],
        [-0.0105, -0.2554, -0.1787,  ...,  0.0280,  0.1750,  0.0908]])
Test Loss      : 0.12215305119752884
Test Acc       : 0.9628